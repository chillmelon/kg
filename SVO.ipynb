{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a900a03d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py2neo in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (2021.2.3)\n",
      "Requirement already satisfied: monotonic in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from py2neo) (1.6)\n",
      "Requirement already satisfied: pansi>=2020.7.3 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from py2neo) (2020.7.3)\n",
      "Requirement already satisfied: six>=1.15.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from py2neo) (1.16.0)\n",
      "Requirement already satisfied: pygments>=2.0.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from py2neo) (2.11.2)\n",
      "Requirement already satisfied: urllib3 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from py2neo) (1.26.12)\n",
      "Requirement already satisfied: certifi in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from py2neo) (2022.9.24)\n",
      "Requirement already satisfied: interchange~=2021.0.4 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from py2neo) (2021.0.4)\n",
      "Requirement already satisfied: packaging in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from py2neo) (21.3)\n",
      "Requirement already satisfied: pytz in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from interchange~=2021.0.4->py2neo) (2022.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from packaging->py2neo) (3.0.9)\n",
      "Requirement already satisfied: wikipedia in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from wikipedia) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\n",
      "Requirement already satisfied: spacy==3.0.3 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (3.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (2.28.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (8.0.17)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (21.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (4.64.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (0.10.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (65.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (3.0.10)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (0.7.9)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (1.7.4)\n",
      "Requirement already satisfied: pathy in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (0.6.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (3.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (1.0.9)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (4.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (2.4.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (4.1.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (2.0.8)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from spacy==3.0.3) (0.3.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.1->spacy==3.0.3) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from packaging>=20.0->spacy==3.0.3) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.3) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.3) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.3) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0.3) (2022.9.24)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from typer<0.4.0,>=0.3.0->spacy==3.0.3) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from jinja2->spacy==3.0.3) (2.1.1)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from pathy->spacy==3.0.3) (5.2.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (1.3.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from pandas) (2022.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from pandas) (1.21.5)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install py2neo\n",
    "!pip install wikipedia\n",
    "!pip install spacy==3.0.3\n",
    "!pip install scikit-learn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e48e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import urllib\n",
    "from pprint import pprint\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from py2neo import Node, Graph, Relationship, NodeMatcher\n",
    "from py2neo.bulk import merge_nodes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wikipedia\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c74c47a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_md\n",
    "# !python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477c7814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n",
      "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer', 'merge_noun_chunks']\n"
     ]
    }
   ],
   "source": [
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "VERBS = ['ROOT', 'advcl']\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
    "ENTITY_LABELS = ['PERSON', 'NORP', 'GPE', 'ORG', 'FAC', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART']\n",
    "\n",
    "\n",
    "non_nc = spacy.load('en_core_web_lg')\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "nlp.add_pipe('merge_noun_chunks')\n",
    "\n",
    "\n",
    "\n",
    "print(non_nc.pipe_names)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b60fc",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0a79e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \n",
    "    regex = re.compile(r'[\\n\\r\\t]')\n",
    "    clean_text = regex.sub(\" \", text)\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_stop_words_and_punct(text, print_text=False):\n",
    "    \n",
    "    result_ls = []\n",
    "    rsw_doc = non_nc(text)\n",
    "    \n",
    "    for token in rsw_doc:\n",
    "        if print_text:\n",
    "            print(token, token.is_stop)\n",
    "            print('--------------')\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            result_ls.append(str(token))\n",
    "    \n",
    "    result_str = ' '.join(result_ls)\n",
    "\n",
    "    return result_str\n",
    "\n",
    "\n",
    "def create_svo_lists(doc, print_lists):\n",
    "    \n",
    "    subject_ls = []\n",
    "    verb_ls = []\n",
    "    object_ls = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ in SUBJECTS:\n",
    "            subject_ls.append((token.lower_, token.idx))\n",
    "        elif token.dep_ in VERBS:\n",
    "            verb_ls.append((token.lemma_, token.idx))\n",
    "        elif token.dep_ in OBJECTS:\n",
    "            object_ls.append((token.lower_, token.idx))\n",
    "\n",
    "    if print_lists:\n",
    "        print('SUBJECTS: ', subject_ls)\n",
    "        print('VERBS: ', verb_ls)\n",
    "        print('OBJECTS: ', object_ls)\n",
    "    \n",
    "    return subject_ls, verb_ls, object_ls\n",
    "\n",
    "\n",
    "def remove_duplicates(tup, tup_posn):\n",
    "    \n",
    "    check_val = set()\n",
    "    result = []\n",
    "    \n",
    "    for i in tup:\n",
    "        if i[tup_posn] not in check_val:\n",
    "            result.append(i)\n",
    "            check_val.add(i[tup_posn])\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_dates(tup_ls):\n",
    "    \n",
    "    clean_tup_ls = []\n",
    "    for entry in tup_ls:\n",
    "        if not entry[2].isdigit():\n",
    "            clean_tup_ls.append(entry)\n",
    "    return clean_tup_ls\n",
    "\n",
    "\n",
    "def create_svo_triples(text, print_lists=False):\n",
    "    \n",
    "    clean_text = remove_special_characters(text)\n",
    "    doc = nlp(clean_text)\n",
    "    subject_ls, verb_ls, object_ls = create_svo_lists(doc, print_lists=print_lists)\n",
    "    \n",
    "    graph_tup_ls = []\n",
    "    dedup_tup_ls = []\n",
    "    clean_tup_ls = []\n",
    "    \n",
    "    for subj in subject_ls: \n",
    "        for obj in object_ls:\n",
    "            \n",
    "            dist_ls = []\n",
    "            \n",
    "            for v in verb_ls:\n",
    "                \n",
    "                # Assemble a list of distances between each object and each verb\n",
    "                dist_ls.append(abs(obj[1] - v[1]))\n",
    "                \n",
    "            # Get the index of the verb with the smallest distance to the object \n",
    "            # and return that verb\n",
    "            index_min = min(range(len(dist_ls)), key=dist_ls.__getitem__)\n",
    "            \n",
    "            # Remve stop words from subjects and object.  Note that we do this a bit\n",
    "            # later down in the process to allow for proper sentence recognition.\n",
    "\n",
    "            no_sw_subj = remove_stop_words_and_punct(subj[0])\n",
    "            no_sw_obj = remove_stop_words_and_punct(obj[0])\n",
    "            \n",
    "            # Add entries to the graph iff neither subject nor object is blank\n",
    "            if no_sw_subj and no_sw_obj:\n",
    "                tup = (no_sw_subj, verb_ls[index_min][0], no_sw_obj)\n",
    "                graph_tup_ls.append(tup)\n",
    "        \n",
    "        #clean_tup_ls = remove_dates(graph_tup_ls)\n",
    "    \n",
    "    dedup_tup_ls = remove_duplicates(graph_tup_ls, 2)\n",
    "    clean_tup_ls = remove_dates(dedup_tup_ls)\n",
    "    \n",
    "    return clean_tup_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764e979",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db2944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obj_properties(tup_ls):\n",
    "    \n",
    "    init_obj_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "\n",
    "        new_tup = (tup[0], tup[1], tup[2])\n",
    "        \n",
    "        init_obj_tup_ls.append(new_tup)\n",
    "        \n",
    "    return init_obj_tup_ls\n",
    "\n",
    "\n",
    "def add_layer(tup_ls):\n",
    "\n",
    "    svo_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        \n",
    "        if tup[3]:\n",
    "            svo_tup = create_svo_triples(tup[3])\n",
    "            svo_tup_ls.extend(svo_tup)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return get_obj_properties(svo_tup_ls)\n",
    "        \n",
    "\n",
    "def subj_equals_obj(tup_ls):\n",
    "    \n",
    "    new_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if tup[0] != tup[2]:\n",
    "            new_tup_ls.append((tup[0], tup[1], tup[2]))\n",
    "            \n",
    "    return new_tup_ls\n",
    "\n",
    "\n",
    "def check_for_string_labels(tup_ls):\n",
    "    # This is for an edge case where the object does not get fully populated\n",
    "    # resulting in the node labels being assigned to string instead of list.\n",
    "    # This may not be strictly necessary and the lines using it are commnted out\n",
    "    # below.  Run this function if you come across this case.\n",
    "    \n",
    "    clean_tup_ls = []\n",
    "    \n",
    "    for el in tup_ls:\n",
    "        if isinstance(el[2], list):\n",
    "            clean_tup_ls.append(el)\n",
    "            \n",
    "    return clean_tup_ls\n",
    "\n",
    "\n",
    "def create_word_vectors(tup_ls):\n",
    "\n",
    "    new_tup_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        new_tup = (tup[0], tup[1], tup[2], np.random.uniform(low=-1.0, high=1.0, size=(300,)))\n",
    "        new_tup_ls.append(new_tup)\n",
    "        \n",
    "    return new_tup_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64cbf88",
   "metadata": {},
   "source": [
    "## Create the node and edge lists to populate the graph with the below helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2174d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(tup_ls):\n",
    "    \n",
    "    visited = set()\n",
    "    output_ls = []\n",
    "    \n",
    "    for tup in tup_ls:\n",
    "        if not tup[0] in visited:\n",
    "            visited.add(tup[0])\n",
    "            output_ls.append((tup[0], tup[1]))\n",
    "            \n",
    "    return output_ls\n",
    "\n",
    "\n",
    "def convert_vec_to_ls(tup_ls):\n",
    "    \n",
    "    vec_to_ls_tup = []\n",
    "    \n",
    "    for el in tup_ls:\n",
    "        vec_ls = [float(v) for v in el[1]]\n",
    "        tup = (el[0], vec_ls)\n",
    "        vec_to_ls_tup.append(tup)\n",
    "        \n",
    "    return vec_to_ls_tup\n",
    "\n",
    "\n",
    "def add_nodes(tup_ls):   \n",
    "\n",
    "    keys = ['name', 'word_vec']\n",
    "    merge_nodes(graph.auto(), tup_ls, ('Node', 'name'), keys=keys)\n",
    "    print('Number of nodes in graph: ', graph.nodes.match('Node').count())\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "113cd7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edges(edge_ls):\n",
    "    \n",
    "    edge_dc = {} \n",
    "    \n",
    "    # Group tuple by verb\n",
    "    # Result: {verb1: [(sub1, v1, obj1), (sub2, v2, obj2), ...],\n",
    "    #          verb2: [(sub3, v3, obj3), (sub4, v4, obj4), ...]}\n",
    "    \n",
    "    for tup in edge_ls: \n",
    "        if tup[1] in edge_dc: \n",
    "            edge_dc[tup[1]].append((tup[0], tup[1], tup[2])) \n",
    "        else: \n",
    "            edge_dc[tup[1]] = [(tup[0], tup[1], tup[2])] \n",
    "    \n",
    "    for edge_labels, tup_ls in tqdm(edge_dc.items()):   # k=edge labels, v = list of tuples\n",
    "        \n",
    "        tx = graph.begin()\n",
    "        \n",
    "        for el in tup_ls:\n",
    "            source_node = nodes_matcher.match(name=el[0]).first()\n",
    "            target_node = nodes_matcher.match(name=el[2]).first()\n",
    "            if not source_node:\n",
    "                source_node = Node('Node', name=el[0])\n",
    "                tx.create(source_node)\n",
    "            if not target_node:\n",
    "                try:\n",
    "                    target_node = Node('Node', name=el[2], word_vec=el[3])\n",
    "                    tx.create(target_node)\n",
    "                except:\n",
    "                    continue\n",
    "            try:\n",
    "                rel = Relationship(source_node, edge_labels, target_node)\n",
    "            except:\n",
    "                continue\n",
    "            tx.create(rel)\n",
    "        tx.commit()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d415de94",
   "metadata": {},
   "source": [
    "## Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e91ed217",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/unsup_data.txt', 'r')\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f27749a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = remove_special_characters(text)\n",
    "doc = nlp(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f48e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean_text.replace('. ', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb9417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.text\n",
    "         for token in doc\n",
    "         if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c19d5c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9939052",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a0b2b76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is made quite clear that the supplier shall be required to fulfill and implement the duties, obligations and responsibilities laid down in the pertinent applicable standard [IATF 16949:2016]',\n",
       " \"Purpose of the document Particular attention is drawn to the following duties and obligations: Purpose of the document The supplier shall determine the customer's requirements in accordance with section 4.3.2 of [IATF 16949:2016] and satisfy them in an effort to improve customer satisfaction\",\n",
       " 'Purpose of the document Among other things, this means that the supplier shall determine requirements that have not been specified by the client but are necessary for the specified or intended use in accordance with section 8.2.2 of [IATF 16949:2016], and shall be obliged to define the properties of the product that are essential to ensure that it can be used safely for its intended purpose in accordance with section 8.2.3 of [IATF 16949:2016]',\n",
       " 'Purpose of the document Furthermore, the supplier shall also use knowledge derived from earlier developments, internal inputs, field data and other suitable sources for similar current and future projects in accordance with section 8.3.3 of [IATF 16949:2016]',\n",
       " 'Purpose of the document As far as inspections and tests are concerned, this implies that the supplier shall ensure that the planned inspections and tests offer an appropriate and adequate means of ensuring performance of development work in accordance with the specifications and delivery of a development object that complies with the specifications',\n",
       " 'Purpose of the document This also applies if the client suggests or demands specific types or numbers of inspections and/or tests',\n",
       " 'Purpose of the document If the supplier is of the opinion that other inspections and/or tests are necessary, he shall advise the client accordingly and perform the pertinent inspections and/or tests',\n",
       " 'Purpose of the document The hardware of the component shall be structured in such a way that failure of functional impacts of important assemblies can be detected via the head unit',\n",
       " 'General This concerns:  Failure/non-plausible values of the temperature sensor  Errors operating voltage measurement path  Failure of backlight LEDs  Failure of voltage supplies  Overvoltage/undervoltage  Touch sensor errors  Failure/non-plausible values of ALS General The elements specified in this section (hardware assemblies, input/output signals, error memory entries, parameters, etc.) General are the minimum scope',\n",
       " 'General Depending on the technical implementation by the supplier, further elements can be necessary']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cbb1a8",
   "metadata": {},
   "source": [
    "## Parse text and build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96411529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_kg(text:str, failed_texts:list, print_lists:bool):\n",
    "    try:\n",
    "        initial_tup_ls = create_svo_triples(text, print_lists=print_lists)\n",
    "        \n",
    "        init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
    "        print(init_obj_tup_ls)\n",
    "        starter_edge_ls = init_obj_tup_ls\n",
    "        edge_ls = subj_equals_obj(starter_edge_ls)\n",
    "        clean_edge_ls = edge_ls\n",
    "\n",
    "        edges_word_vec_ls = create_word_vectors(edge_ls)\n",
    "\n",
    "        orig_node_tup_ls = [(edge_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
    "        obj_node_tup_ls = [(tup[2], tup[3]) for tup in edges_word_vec_ls]\n",
    "        full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
    "        dedup_node_tup_ls = dedup(full_node_tup_ls)\n",
    "\n",
    "        node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)\n",
    "\n",
    "        add_nodes(node_tup_ls)\n",
    "        add_edges(edges_word_vec_ls)\n",
    "    except:\n",
    "        failed_texts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636387f",
   "metadata": {},
   "source": [
    "## Connect to neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d2debf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(\"bolt://localhost:7999\", name=\"neo4j\", password=\"secret\")\n",
    "nodes_matcher = NodeMatcher(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d4b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "failed_texts = []\n",
    "for t in rows:\n",
    "    text_to_kg(t, failed_texts, print_lists=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe49f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(failed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1233ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "failed_texts = []\n",
    "for t in failed_texts:\n",
    "    text_to_kg(t, [], print_lists=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad30a447",
   "metadata": {},
   "source": [
    "## ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0a917",
   "metadata": {},
   "source": [
    "## Start Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ac0784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Particular attention is drawn to the following duties and obligations: Purpose of the document'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5effa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('particular attention', 'draw', 'following duties'), ('particular attention', 'purpose', 'document')]\n",
      "Number of nodes in graph:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                       | 0/2 [00:00<?, ?it/s]/usr/local/Caskroom/miniconda/base/envs/kg2/lib/python3.7/site-packages/ipykernel_launcher.py:36: DeprecationWarning: The transaction.commit() method is deprecated, use graph.commit(transaction) instead\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 43.60it/s]\n"
     ]
    }
   ],
   "source": [
    "text_to_kg(text, [], print_lists=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8c2ba39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.3 ms, sys: 2.01 ms, total: 28.3 ms\n",
      "Wall time: 27.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "initial_tup_ls = create_svo_triples(text, print_lists=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c203d624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('particular attention', 'draw', 'following duties'),\n",
       " ('particular attention', 'purpose', 'document')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tup_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31b2d856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('particular attention', 'draw', 'following duties'), ('particular attention', 'purpose', 'document')]\n",
      "CPU times: user 172 µs, sys: 69 µs, total: 241 µs\n",
      "Wall time: 224 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "init_obj_tup_ls = get_obj_properties(initial_tup_ls)\n",
    "print(init_obj_tup_ls)\n",
    "starter_edge_ls = init_obj_tup_ls\n",
    "edge_ls = subj_equals_obj(starter_edge_ls)\n",
    "clean_edge_ls = edge_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e191eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_word_vec_ls = create_word_vectors(edge_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600bbed",
   "metadata": {},
   "source": [
    "## Creating some lists of tuples representing the node and edge lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d326695c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orig_node_tup_ls = [(edge_ls[0][0], '', ['Subject'], '', np.random.uniform(low=-1.0, high=1.0, size=(300,)))]\n",
    "obj_node_tup_ls = [(tup[2], tup[3]) for tup in edges_word_vec_ls]\n",
    "full_node_tup_ls = orig_node_tup_ls + obj_node_tup_ls\n",
    "dedup_node_tup_ls = dedup(full_node_tup_ls)\n",
    "\n",
    "len(full_node_tup_ls), len(dedup_node_tup_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbdd691",
   "metadata": {},
   "source": [
    "## Create the node list that will be used to populate the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_tup_ls = convert_vec_to_ls(dedup_node_tup_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb83df",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(\"bolt://localhost:7999\", name=\"neo4j\", password=\"secret\")\n",
    "nodes_matcher = NodeMatcher(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "add_nodes(node_tup_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "add_edges(edges_word_vec_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23313d8",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5cb651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vec_similarity(node1, node2, node_ls):\n",
    "    \n",
    "    node1_vec = [tup[4] for tup in node_ls if tup[0] == node1]\n",
    "    node2_vec = [tup[4] for tup in node_ls if tup[0] == node2]\n",
    "    \n",
    "    return cosine_similarity(node1_vec, node2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b147399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = get_word_vec_similarity('company', 'standard', dedup_node_tup_ls)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d59f68",
   "metadata": {},
   "source": [
    "## Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n",
    "\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6990b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(\"./data/test.pdf\")\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    text += page.extract_text() + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69379bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reader.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed4585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = reader.pages[20].extract_text()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
